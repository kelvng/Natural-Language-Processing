{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nthanhkhang/Natural-Language-Processing/blob/main/beam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHM1PsLj4OHq"
   },
   "source": [
    "# Thực hành với Beam search\n",
    "\n",
    "Khôi phục dấu câu tiếng Việt sử dụng Beam search & Language model\n",
    "\n",
    "> Click vào ảnh để xem bài hướng dẫn chi tiết\n",
    "\n",
    "[![](https://nguyenvanhieu.vn/wp-content/uploads/2020/07/beam-search.jpg)](https://nguyenvanhieu.vn/thuat-toan-beam-search/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v_20ohjigege"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import itertools\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L662vkyPgs67"
   },
   "outputs": [],
   "source": [
    "# Xóa dấu tiếng Việt\n",
    "def remove_vn_accent(s):\n",
    "    s = re.sub('[áàảãạăắằẳẵặâấầẩẫậ]', 'a', s)\n",
    "    s = re.sub('[éèẻẽẹêếềểễệ]', 'e', s)\n",
    "    s = re.sub('[óòỏõọôốồổỗộơớờởỡợ]', 'o', s)\n",
    "    s = re.sub('[íìỉĩị]', 'i', s)\n",
    "    s = re.sub('[úùủũụưứừửữự]', 'u', s)\n",
    "    s = re.sub('[ýỳỷỹỵ]', 'y', s)\n",
    "    s = re.sub('đ', 'd', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1hBPibDgyJt",
    "outputId": "85141dd8-8415-45de-b63d-9dcc976e8502"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download VN syllables\n",
    "!wget -O vn_syllables.txt \"https://gist.githubusercontent.com/nguyenvanhieuvn/8d7c3440590a6db732ef6e05498c1566/raw/0164ccb7094b22a48a52844e7fa748cf2820cec9/all-vietnamese-syllables.txt(G%25C3%25B5%2520d%25E1%25BA%25A5u%2520ki%25E1%25BB%2583u%2520c%25C5%25A9)\"\n",
    "# Download language model\n",
    "!wget \"https://github.com/nguyenvanhieuvn/vn-accent-resoration/raw/master/vn_en_nextwords.txt.zip\"\n",
    "!unzip vn_en_nextwords.txt.zip\n",
    "# Download test data\n",
    "!wget \"https://github.com/nguyenvanhieuvn/vn-accent-resoration/raw/master/test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kJxEZ3P9hQGK"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vn_syllables.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2a59512b9c10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Tạo bộ từ điển sinh dấu câu cho các từ không dấu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmap_accents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vn_syllables.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mno_accent_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_vn_accent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vn_syllables.txt'"
     ]
    }
   ],
   "source": [
    "# Tạo bộ từ điển sinh dấu câu cho các từ không dấu\n",
    "map_accents = {}\n",
    "for word in open('vn_syllables.txt').read().splitlines():\n",
    "  word = word.lower()\n",
    "  no_accent_word = remove_vn_accent(word)\n",
    "  if no_accent_word not in map_accents:\n",
    "    map_accents[no_accent_word] = set()\n",
    "  map_accents[no_accent_word].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbrvxOJLiKO7"
   },
   "outputs": [],
   "source": [
    "print(map_accents['chac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFDH2rCRqCKi"
   },
   "outputs": [],
   "source": [
    "# Đọc lm\n",
    "lm = {}\n",
    "for line in open('vn_en_nextwords.txt'):\n",
    "  data = json.loads(line)\n",
    "  key = data['s']\n",
    "  lm[key] = data\n",
    "vocab_size = len(lm)\n",
    "total_word = 0\n",
    "for word in lm:\n",
    "  total_word += lm[word]['sum']\n",
    "print(vocab_size, total_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S55LZeC_5HMD"
   },
   "outputs": [],
   "source": [
    "lm['khang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFUYIbcU9LZI"
   },
   "outputs": [],
   "source": [
    "# tính xác suất dùng smoothing\n",
    "def get_proba(current_word, next_word):\n",
    "  if current_word not in lm:\n",
    "    return 1 / total_word;\n",
    "  if next_word not in lm[current_word]['next']:\n",
    "    return 1 / (lm[current_word]['sum'] + vocab_size)\n",
    "  return (lm[current_word]['next'][next_word] + 1) / (lm[current_word]['sum'] + vocab_size)\n",
    "\n",
    "# def get_proba(current_word, next_word):\n",
    "#   try:\n",
    "#     return (lm[current_word]['next'][next_word]) / (lm[current_word]['sum'])\n",
    "#   except:\n",
    "#     return 1e-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBQlSHgR34-h"
   },
   "outputs": [],
   "source": [
    "# hàm beam search\n",
    "def beam_search(words, k=3):\n",
    "  sequences = []\n",
    "  for idx, word in enumerate(words):\n",
    "    if idx == 0:\n",
    "      sequences = [([x], 0.0) for x in map_accents.get(word, [word])]\n",
    "    else:\n",
    "      all_sequences = []\n",
    "      for seq in sequences:\n",
    "        for next_word in map_accents.get(word, [word]):\n",
    "          current_word = seq[0][-1]\n",
    "          proba = get_proba(current_word, next_word)\n",
    "          # print(current_word, next_word, proba, log(proba))\n",
    "          proba = log(proba)\n",
    "          new_seq = seq[0].copy()\n",
    "          new_seq.append(next_word)\n",
    "          all_sequences.append((new_seq, seq[1] + proba))\n",
    "      # print(all_sequences) \n",
    "      all_sequences = sorted(all_sequences,key=lambda x: x[1], reverse=True)\n",
    "      sequences = all_sequences[:k]\n",
    "  return sequences\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko_N1RvRsfbV"
   },
   "outputs": [],
   "source": [
    "# tiền xử lý text\n",
    "def preprocess(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = re.sub(r'[.,~`!@#$%\\^&*\\(\\)\\[\\]\\\\|:;\\'\"]+', ' ', sentence)\n",
    "  sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARdWnejy38xn",
    "outputId": "17337b0f-33e6-41a2-cf50-455d77c76d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INP: tuy nhiên giai đoạn từ 2009 đến ngày 175/10/2017 công ty không còn thực hiện các trang\n",
      "OUT: tuy nhiên giai đoạn từ 2009 đến ngày 175/10/2017 công ty không còn thực hiện các trang\n",
      "CMP: True\n"
     ]
    }
   ],
   "source": [
    "# test 1 câu\n",
    "sentence = \"Tuy nhiên giai đoạn từ 2009 đến ngày 17/10/2017 cong ty không còn thực hiện các  trang\"\n",
    "sentence = preprocess(sentence)\n",
    "_sentence = remove_vn_accent(sentence)\n",
    "words = _sentence.split()\n",
    "results = beam_search(words, k=5)\n",
    "print('INP:', sentence)\n",
    "\n",
    "print('OUT:', ' '.join(results[0][0]))\n",
    "print('CMP:', ' '.join(results[0][0]) == sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ87WWXQzQOe"
   },
   "outputs": [],
   "source": [
    "# test qua bộ test ~ 5000 câu\n",
    "k = 10\n",
    "sentences = open('test.txt').read().splitlines()\n",
    "test_size = len(sentences)\n",
    "print(test_size)\n",
    "correct = 0\n",
    "for sent in sentences:\n",
    "  try:\n",
    "    sent = preprocess(sent)\n",
    "    _sent = remove_vn_accent(sent)\n",
    "    words = _sent.split()\n",
    "    results = beam_search(words, k)\n",
    "    if ' '.join(results[0][0]) == sent:\n",
    "      correct += 1\n",
    "  except:\n",
    "    print('err', sent)\n",
    "    break\n",
    "\n",
    "print(correct / test_size)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCpzbFlw2PrL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "beam_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
