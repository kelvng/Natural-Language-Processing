{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\nguyn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\nguyn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre NOUN\n",
      "Vinken NOUN\n",
      ", .\n",
      "61 NUM\n",
      "years NOUN\n",
      "old ADJ\n",
      ", .\n",
      "will VERB\n",
      "join VERB\n",
      "the DET\n",
      "board NOUN\n",
      "as ADP\n",
      "a DET\n",
      "nonexecutive ADJ\n",
      "director NOUN\n",
      "Nov. NOUN\n",
      "29 NUM\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "sent = nltk_data[0]\n",
    "for i in range(len(sent)):\n",
    "    print(sent[i][0],sent[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3914\n",
      "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "[('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(len(nltk_data))\n",
    "for i in range(2):\n",
    "    print(nltk_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131\n",
      "783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation set in the ratio 80:20\n",
    "train_set,test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state=101)\n",
    "# create list of train and test tagged words\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. tính tham số model\n",
    "# P(tj|ti) ->log\n",
    "# P(wi|tj) ->log\n",
    "#2. thuật toán Viterbi\n",
    "#3. test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(sent):\n",
    "    tag_gram = []\n",
    "    tagtag_gram = []\n",
    "    tagword_gram = []\n",
    "    for i in range(len(sent)):\n",
    "        if i==0:\n",
    "            tag_gram.append('T0')\n",
    "            tagtag = 'T0'+'_'+sent[i][1]\n",
    "        else:\n",
    "            tagtag = sent[i-1][1]+'_'+sent[i][1]\n",
    "        tag_gram.append(sent[i][1])\n",
    "        tagword = sent[i][1]+'_'+sent[i][0] \n",
    "        tagtag_gram.append(tagtag)\n",
    "        tagword_gram.append(tagword)\n",
    "        #print(sent[i][0])\n",
    "    return tag_gram,tagtag_gram,tagword_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T0', 'NOUN', 'NOUN', 'VERB', 'ADP', 'NOUN']\n",
      "['T0_NOUN', 'NOUN_NOUN', 'NOUN_VERB', 'VERB_ADP', 'ADP_NOUN']\n",
      "['NOUN_Drink', 'NOUN_Carrier', 'VERB_Competes', 'ADP_With', 'NOUN_Cartons']\n"
     ]
    }
   ],
   "source": [
    "tag_gram, tagtag_gram, tagword_gram = get_ngram(train_set[0])\n",
    "print(tag_gram)\n",
    "print(tagtag_gram)\n",
    "print(tagword_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_gram_all=[]\n",
    "tagtag_gram_all=[]\n",
    "tagword_gram_all=[]\n",
    "for i in range(len(train_set)):\n",
    "    tag_gram, tagtag_gram, tagword_gram = get_ngram(train_set[i])\n",
    "    tag_gram_all.extend(tag_gram)\n",
    "    tagtag_gram_all.extend(tagtag_gram)\n",
    "    tagword_gram_all.extend(tagword_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83441\n",
      "80310\n",
      "80310\n"
     ]
    }
   ],
   "source": [
    "print(len(tag_gram_all))\n",
    "print(len(tagtag_gram_all))\n",
    "print(len(tagword_gram_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 22966, 'VERB': 10860, '.': 9321, 'ADP': 7902, 'DET': 6957, 'X': 5203, 'ADJ': 5150, 'T0': 3131, 'NUM': 2801, 'ADV': 2578, 'PRT': 2555, 'PRON': 2195, 'CONJ': 1822})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "counter.update(tag_gram_all)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 22966), ('VERB', 10860), ('.', 9321), ('ADP', 7902), ('DET', 6957), ('NOUN_NOUN', 6021), ('NOUN_.', 5512), ('X', 5203), ('ADJ', 5150), ('DET_NOUN', 4424), ('NOUN_ADP', 4059), ('ADJ_NOUN', 3589), ('NOUN_VERB', 3425), ('T0', 3131), ('NUM', 2801), ('ADV', 2578), ('ADP_NOUN', 2556), ('PRT', 2555), ('ADP_DET', 2536), ('VERB_X', 2345), ('PRON', 2195), ('VERB_VERB', 1824), ('CONJ', 1822), ('VERB_DET', 1451), ('DET_ADJ', 1436), ('VERB_NOUN', 1201), ('._NOUN', 1141), ('X_VERB', 1074), ('PRON_VERB', 1064), ('PRT_VERB', 1025)]\n"
     ]
    }
   ],
   "source": [
    "counter.update(tagtag_gram_all)\n",
    "print(counter.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 22966), ('VERB', 10860), ('.', 9321), ('ADP', 7902), ('DET', 6957), ('NOUN_NOUN', 6021), ('NOUN_.', 5512), ('X', 5203), ('ADJ', 5150), ('DET_NOUN', 4424), ('NOUN_ADP', 4059), ('._,', 3878), ('._.', 3679), ('ADJ_NOUN', 3589), ('NOUN_VERB', 3425), ('DET_the', 3231), ('T0', 3131), ('NUM', 2801), ('ADV', 2578), ('ADP_NOUN', 2556)]\n"
     ]
    }
   ],
   "source": [
    "counter.update(tagword_gram_all)\n",
    "print(counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-d518f270",
   "language": "python",
   "display_name": "PyCharm (Natural-Language-Processing)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}